{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Portuguese to English.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcHuJU9bVNcELSJXR6eusk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satwikram/Transformer-model-for-language-understanding/blob/master/Portuguese%20to%20English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZcl6PM2v3IH",
        "colab_type": "text"
      },
      "source": [
        "## Author: Satwik Ram K\n",
        "Portuguese to English Transalation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsQ3MjisxLyH",
        "colab_type": "text"
      },
      "source": [
        "## Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWeRaJ42xPXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBG44wtuxzVe",
        "colab_type": "text"
      },
      "source": [
        "## Loading Dataset\n",
        "\n",
        "The dataset is available in Tfds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNad1bVkxgcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, metadata = tfds.load('ted_hrlr_translate/pt_to_en',with_info = True, as_supervised=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb-bYBX9yNRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2776d3ae-ca84-4971-edfc-881d6953e660"
      },
      "source": [
        "data"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>,\n",
              " 'train': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>,\n",
              " 'validation': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nJvKw2eyTBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b583baf1-6c68-488f-a2c5-d1fab24b6460"
      },
      "source": [
        "metadata"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='ted_hrlr_translate',\n",
              "    version=1.0.0,\n",
              "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
              "where one is high resource and the other is low resource.\n",
              "',\n",
              "    homepage='https://github.com/neulab/word-embeddings-for-nmt',\n",
              "    features=Translation({\n",
              "        'en': Text(shape=(), dtype=tf.string),\n",
              "        'pt': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=54781,\n",
              "    splits={\n",
              "        'test': 1803,\n",
              "        'train': 51785,\n",
              "        'validation': 1193,\n",
              "    },\n",
              "    supervised_keys=('pt', 'en'),\n",
              "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
              "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
              "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
              "      booktitle = {HLT-NAACL},\n",
              "      year    = {2018},\n",
              "      }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTfhvZD0ysDt",
        "colab_type": "text"
      },
      "source": [
        "## Taking Train and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUAPgtOzyVpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, val = data['train'], data['validation']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGeetyXly5Zq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cfa8f2f-50f5-43fb-e8fd-3cfdf36ce46e"
      },
      "source": [
        "type(train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZejYX5k2DCW",
        "colab_type": "text"
      },
      "source": [
        "## Creating a custom subwords tokenizer from the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFAUITGu0hyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "0249b022-8f87-4d6e-df2e-ed63525f7586"
      },
      "source": [
        "count = 0\n",
        "for a, b in train:\n",
        "  count += 1\n",
        "  if count == 10:\n",
        "    break\n",
        "  else:\n",
        "    print(\"Portugees sentence:\",a)\n",
        "    print(\"English Corresponding Sentence\",b)\n",
        "    print(\"--\"*60)\n",
        "  "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portugees sentence: tf.Tensor(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'mas e se estes fatores fossem ativos ?', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b'but what if it were active ?', shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'mas eles n\\xc3\\xa3o tinham a curiosidade de me testar .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b\"but they did n't test for curiosity .\", shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'e esta rebeldia consciente \\xc3\\xa9 a raz\\xc3\\xa3o pela qual eu , como agn\\xc3\\xb3stica , posso ainda ter f\\xc3\\xa9 .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b'and this conscious defiance is why i , as an agnostic , can still have faith .', shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b\"`` `` '' podem usar tudo sobre a mesa no meu corpo . ''\", shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b'you can use everything on the table on me .', shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b\"`` eu escrevo muito acerca do `` '' teatro de seguran\\xc3\\xa7a '' '' , que s\\xc3\\xa3o produtos que fazem as pessoas sentirem-se seguras mas que , na realidade , n\\xc3\\xa3o fazem nada . ''\", shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b\"`` i write a lot about `` '' security theater , '' '' which are products that make people feel secure , but do n't actually do anything . ''\", shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'colocaram-no bem no fundo duma mina de ferro no minnesota , nos \\xc3\\xbaltimos dois dias anunciaram os resultados mais sens\\xc3\\xadveis at\\xc3\\xa9 agora .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b\"and they 've put it deep down in an iron mine in minnesota , ok , deep under the ground , and in fact , in the last couple of days announced the most sensitive results so far .\", shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'algumas pessoas t\\xc3\\xaam medo de que n\\xc3\\xa3o gostem delas .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b'see , some people might fear girls not liking them back .', shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Portugees sentence: tf.Tensor(b'n\\xc3\\xa3o , o que nos aconteceu , chris , \\xc3\\xa9 que o poder , o pre\\xc3\\xa7o est\\xc3\\xa1 fixado fora da margem .', shape=(), dtype=string)\n",
            "English Corresponding Sentence tf.Tensor(b\"no , what happened to us , chris , is that power , it 's priced off the margin .\", shape=(), dtype=string)\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZnCUBg0-lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train), target_vocab_size = 2**13\n",
        ")\n",
        "\n",
        "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train), target_vocab_size = 2**13\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2fZdZT05NFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en.save_to_file('tokenizer_en')\n",
        "tokenizer_pt.save_to_file('tokenizer_pt')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWv6RudK6SG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading saved tokenizer\n",
        "encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file('/content/tokenizer_en')\n",
        "encoder_pt = tfds.features.text.SubwordTextEncoder.load_from_file('/content/tokenizer_pt')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Ci-MAs6pNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "989391d3-ac69-4d24-dc44-dc365f437d42"
      },
      "source": [
        "sample_string = \"Transfomer is cool.\"\n",
        "\n",
        "encoded_string = encoder_en.encode(sample_string)\n",
        "\n",
        "original_string = encoder_en.decode(encoded_string)\n",
        "\n",
        "print(\"Orginal String:\",sample_string)\n",
        "print(\"Encoded String:\",encoded_string)\n",
        "print(\"Decoded String:\",original_string)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Orginal String: Transfomer is cool.\n",
            "Encoded String: [7915, 1248, 7946, 1391, 2762, 7863, 13, 1671, 7877]\n",
            "Decoded String: Transfomer is cool.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59fXhQCg7ivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0nORr1m8MxF",
        "colab_type": "text"
      },
      "source": [
        "## Add a start and end token to the input and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCKxrFDSATqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e809cefd-a25d-4abd-ec4b-b67da5d43bd5"
      },
      "source": [
        "print(encoder_pt.vocab_size)\n",
        "print(encoder_en.vocab_size)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8214\n",
            "8087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRsPfOQIB_2o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cebd646f-93ee-482b-eba4-ab35de1c3420"
      },
      "source": [
        "a = [encoder_pt.vocab_size]\n",
        "b = np.array([2])\n",
        "print(b[0])\n",
        "c = a + b[0]\n",
        "print(c)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "[8216]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZhkwg0H_Llv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [encoder_pt.vocab_size] + encoder_pt.encode(lang1.numpy()) + [encoder_pt.vocab_size + 1]\n",
        "\n",
        "  lang2 = [encoder_en.vocab_size] + encoder_en.encode(lang2.numpy()) + [encoder_en.vocab_size + 1]\n",
        "\n",
        "  return lang1, lang2"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsJaq_72C38c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(pt, en):\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UahS8-ajDtkQ",
        "colab_type": "text"
      },
      "source": [
        "## Dropping long sentences over 40 tokens "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EJCbbZ3DmRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELwafB1PDqm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y, max_length = MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d3-4Mv8E3iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = val.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLMLKiytJMmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7d21c766-266d-4987-af9a-c0bf61dbc4ec"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "pt_batch, en_batch"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
              " array([[8214,  342, 3032, ...,    0,    0,    0],\n",
              "        [8214,   95,  198, ...,    0,    0,    0],\n",
              "        [8214, 4479, 7990, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8214,  584,   12, ...,    0,    0,    0],\n",
              "        [8214,   59, 1548, ...,    0,    0,    0],\n",
              "        [8214,  118,   34, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
              " array([[8087,   98,   25, ...,    0,    0,    0],\n",
              "        [8087,   12,   20, ...,    0,    0,    0],\n",
              "        [8087,   12, 5453, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8087,   18, 2059, ...,    0,    0,    0],\n",
              "        [8087,   16, 1436, ...,    0,    0,    0],\n",
              "        [8087,   15,   57, ...,    0,    0,    0]])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiCuB_wlJYSr",
        "colab_type": "text"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzn3s3m1JRgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angles_rates = 1 / np.power(1000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angles_rates"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_-Q5dszdkoE",
        "colab_type": "text"
      },
      "source": [
        "np.newaxis will expand the dimension of the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch0aUwr9atf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_rm_v1ebXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4ade1cd-f5f3-45f4-883b-ba12ea0e88a1"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print(pos_encoding.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4pSrNOKgmj8",
        "colab_type": "text"
      },
      "source": [
        "## Masking\n",
        "\n",
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoKZwyTGgTAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tCi67dti9kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fb2835f4-04ca-4163-b8e6-5f0de25eb319"
      },
      "source": [
        "xx = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(xx)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ_DRjPZjLw5",
        "colab_type": "text"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwFBQ7UqjE_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nK_rNV2k9aU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9dd6536a-f3be-4b7a-81f6-e35ef88ae9b5"
      },
      "source": [
        "xx = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(xx.shape[1])\n",
        "temp  "
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLlynz8lmZAj",
        "colab_type": "text"
      },
      "source": [
        "## Scaled Dot Product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-7YtzLlEEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "\n",
        "  matmul_qk = tf.matmul(q, v, transpose_b = True)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # Applying softmax\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBpL7H52rvpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqkKxhWys8gv",
        "colab_type": "text"
      },
      "source": [
        "## Multi-head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl0RemwVs7bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}